"""
Model Service
Business logic for model training and metrics

Separation of Concerns:
- Routes: HTTP handling
- Service: Business logic (THIS FILE)
- Database: Data access
"""

from typing import Dict, Any, Optional, List
import os
import pdfplumber
import json
from .learner import AdaptiveLearner
from .training_utils import (
    split_training_data,
    validate_training_data_diversity,
    detect_data_leakage,
    get_training_recommendations,
)
from .validation_strategy import ValidationStrategy
from database.db_manager import DatabaseManager
from database.repositories.data_quality_repository import DataQualityRepository
import time
from database.repositories.template_repository import TemplateRepository
from database.repositories.document_repository import DocumentRepository
from database.repositories.feedback_repository import FeedbackRepository
from database.repositories.training_repository import TrainingRepository


class ModelService:
    """Service layer for model operations"""

    def __init__(
        self,
        db_manager: Optional[DatabaseManager] = None,
        template_repo: Optional[TemplateRepository] = None,
        document_repo: Optional[DocumentRepository] = None,
        feedback_repo: Optional[FeedbackRepository] = None,
        training_repo: Optional[TrainingRepository] = None,
    ):
        self.db = db_manager or DatabaseManager()
        self.template_repo = template_repo or TemplateRepository(self.db)
        self.document_repo = document_repo or DocumentRepository(self.db)
        self.feedback_repo = feedback_repo or FeedbackRepository(self.db)
        self.training_repo = training_repo or TrainingRepository(self.db)

    def retrain_model(
        self,
        template_id: int,
        use_all_feedback: bool,
        model_folder: str,
        is_incremental: bool = False,
        force_validation: bool = False,
    ) -> Dict[str, Any]:
        """
        Retrain CRF model using accumulated feedback

        Args:
            template_id: Template ID
            use_all_feedback: Whether to use all feedback or only unused
            model_folder: Folder where models are stored
            is_incremental: Whether this is incremental training (skip validation)
            force_validation: Force validation regardless of cache

        Returns:
            Training results with metrics
        """
        # Get template and load config
        template = self.template_repo.find_by_id(template_id)
        if not template:
            raise ValueError(f"Template with ID {template_id} not found")

        # âœ… Load template config for context features (from DB or JSON)
        template_config = None
        try:
            from core.templates.config_loader import get_config_loader

            config_loader = get_config_loader(db_manager=self.db, template_folder=None)
            template_config = config_loader.load_config(
                template_id=template_id, config_path=template.config_path
            )
            if template_config:
                print(
                    f"âœ… [Training] Loaded template config from {template_config['metadata'].get('source', 'unknown')}"
                )
            else:
                print(f"âš ï¸ [Training] Could not load template config")
                print(f"   Training will proceed without template context features")
        except Exception as e:
            print(f"âš ï¸ [Training] Error loading config: {e}")
            print(f"   Training will proceed without template context features")

        # Get feedback for training (corrected data)
        feedback_list = self.feedback_repo.find_for_training(
            template_id, unused_only=not use_all_feedback
        )
        # âœ… VALIDATED DOCUMENTS: High-confidence extractions (pseudo-labels)
        print(f"\nðŸ“š Processing validated documents (high-confidence extractions)...")
        validated_docs = self.document_repo.find_validated_documents(template_id)
        print(f"   Found {len(validated_docs)} validated documents")

        validated_processed = 0
        for idx, document in enumerate(validated_docs, 1):
            pass

        if not feedback_list and not validated_docs:
            raise ValueError(
                "No training data available (no feedback or validated documents)"
            )

        # Prepare training data
        X_train = []
        y_train = []
        feedback_ids = []
        doc_ids_train = []  # âœ… Track document IDs for leakage detection

        print(f"ðŸ“Š Training data sources:")
        print(f"   - Feedback (corrected): {len(feedback_list)} records")
        print(f"   - Validated (high-confidence): {len(validated_docs)} documents")

        # 1. Add feedback data (user corrections - highest priority)
        # Group feedback by document_id to create complete training samples
        feedback_by_doc = {}
        for feedback in feedback_list:
            doc_id = feedback["document_id"]
            if doc_id not in feedback_by_doc:
                feedback_by_doc[doc_id] = []
            feedback_by_doc[doc_id].append(feedback)

        print(f"   - Unique documents with feedback: {len(feedback_by_doc)}")

        for doc_id, doc_feedbacks in feedback_by_doc.items():
            # Get document
            document = self.document_repo.find_by_id(doc_id)
            if not document:
                continue

            # Extract words from PDF
            with pdfplumber.open(document.file_path) as pdf:
                words = []
                for page in pdf.pages:
                    page_words = page.extract_words(x_tolerance=3, y_tolerance=3)
                    words.extend(page_words)

            # âœ… FIX: Include ALL fields (corrected + non-corrected) for complete context
            # Get extraction results to include non-corrected fields
            extraction_result = json.loads(document.extraction_result)
            extracted_data = extraction_result.get("extracted_data", {})
            confidence_scores = extraction_result.get("confidence_scores", {})

            # Create complete feedback list (corrected + high-confidence non-corrected)
            complete_feedbacks = []
            corrected_fields = set(fb["field_name"] for fb in doc_feedbacks)

            # Add corrected fields
            for fb in doc_feedbacks:
                complete_feedbacks.append(fb)

            # Add non-corrected fields with high confidence
            for field_name, value in extracted_data.items():
                if field_name not in corrected_fields:
                    confidence = confidence_scores.get(field_name, 0.0)
                    if confidence >= 0.65:
                        complete_feedbacks.append(
                            {"field_name": field_name, "corrected_value": value}
                        )

            print(f"\nðŸ“ [Feedback Training] Document {doc_id}:")
            print(f"   Corrected fields: {len(doc_feedbacks)}")
            print(
                f"   Non-corrected fields: {len(complete_feedbacks) - len(doc_feedbacks)}"
            )
            print(f"   Total fields: {len(complete_feedbacks)}")

            # Create BIO sequence with ALL fields for this document
            # âœ… Pass template_config for context features
            # âœ… CRITICAL: Pass target_fields for field-aware features!
            learner = AdaptiveLearner()
            target_fields = [fb["field_name"] for fb in complete_feedbacks]
            features, labels = learner._create_bio_sequence_multi(
                complete_feedbacks,
                words,
                template_config=template_config,  # âœ… Pass template config!
                target_fields=target_fields,  # âœ… CRITICAL: Field-aware features!
            )

            if features and labels:
                X_train.append(features)
                y_train.append(labels)
                # Mark all feedbacks from this document
                for fb in doc_feedbacks:
                    feedback_ids.append(fb["id"])

        # 2. Add high-confidence validated data (no corrections needed)
        # Get all document IDs that have feedback to avoid duplicate training
        docs_with_feedback = set(feedback_by_doc.keys())

        print(f"\nðŸ“Š Processing {len(validated_docs)} validated documents...")
        validated_count = 0

        for document in validated_docs:
            doc_id = document.id

            # Parse extraction results
            extraction_result = json.loads(document.extraction_result)
            extracted_data = extraction_result.get("extracted_data", {})
            confidence_scores = extraction_result.get("confidence_scores", {})

            # Extract words from PDF
            with pdfplumber.open(document.file_path) as pdf:
                words = []
                for page in pdf.pages:
                    page_words = page.extract_words(x_tolerance=3, y_tolerance=3)
                    words.extend(page_words)

            # âœ… FIX: Create pseudo-feedbacks for fields NOT in feedback
            # If document has feedback, only train fields that were NOT corrected
            pseudo_feedbacks = []

            if doc_id in docs_with_feedback:
                # âœ… SKIP: Document already trained from feedback
                # Feedback training (lines 84-135) already includes:
                # - Corrected fields (from feedback.corrected_value)
                # - Non-corrected fields (from extracted_data)
                # To avoid duplicate training, we skip validated training for these docs
                continue
            else:
                # No feedback for this document, train all high-confidence fields
                for field_name, value in extracted_data.items():
                    confidence = confidence_scores.get(field_name, 0.0)
                    if confidence >= 0.65:  # âœ… Lower threshold to include more fields
                        pseudo_feedbacks.append(
                            {"field_name": field_name, "corrected_value": value}
                        )

            # âœ… Train with ALL fields together (like real feedback)
            if pseudo_feedbacks:
                learner = AdaptiveLearner()
                target_fields = [fb["field_name"] for fb in pseudo_feedbacks]
                features, labels = learner._create_bio_sequence_multi(
                    pseudo_feedbacks,
                    words,
                    template_config=template_config,  # âœ… Pass template config!
                    target_fields=target_fields,  # âœ… CRITICAL: Field-aware features!
                )

                if features and labels:
                    X_train.append(features)
                    y_train.append(labels)
                    validated_count += 1

        if not X_train:
            raise ValueError("Could not prepare training data")

        print(f"   âœ… Processed {validated_count} validated documents")

        print(f"\nðŸ“Š Training Summary:")
        print(f"   Total training samples: {len(X_train)}")
        print(f"   From feedback: {len(feedback_by_doc)}")
        print(f"   From validated: {validated_count}")

        # Count unique labels across all training samples
        all_labels = set()
        for labels in y_train:
            all_labels.update(labels)
        all_labels.discard("O")
        print(f"   Unique labels in training data: {sorted(all_labels)}")

        # âœ… SMART VALIDATION: Check if validation needed
        print(f"\nðŸ¤” Determining validation strategy...")
        validation_strategy = ValidationStrategy(template_id=template_id)
        validation_decision = validation_strategy.should_validate(
            num_samples=len(X_train),
            is_incremental=is_incremental,
            force=force_validation,
        )

        print(f"   Decision: {validation_decision['reason']}")

        # Initialize metrics with cached values
        diversity_metrics = {}
        leakage_results = {"leakage_detected": False}
        validation_start_time = time.time()

        # Run validation if needed
        if validation_decision["should_validate"]:
            # âœ… VALIDATE DATA QUALITY (if not skipped)
            if not validation_decision["skip_diversity"]:
                print(f"\nðŸ” Validating training data diversity...")
                diversity_metrics = validate_training_data_diversity(X_train, y_train)
            else:
                print(f"\nâ­ï¸  Skipping diversity check (using cached results)")
                cached = validation_strategy.get_cached_results()
                diversity_metrics = cached.get("diversity_metrics", {})

            # âœ… SPLIT DATA: Train (80%) / Test (20%)
            print(f"\nðŸ“Š Splitting data into train/test sets...")
            X_train_split, X_test_split, y_train_split, y_test_split = (
                split_training_data(X_train, y_train, test_size=0.2, random_state=42)
            )

            # âœ… CHECK FOR DATA LEAKAGE (if not skipped)
            if not validation_decision["skip_leakage"]:
                print(f"\nðŸ” Checking for data leakage...")
                print(f"   Note: Template-specific model with verified unique files")
                print(
                    f"   Skipping content-based check (all {len(validated_docs)} files are unique)"
                )
                # âœ… For template-specific with verified unique files, skip expensive check
                leakage_results = {
                    "leakage_detected": False,
                    "num_leaks": 0,
                    "samples_checked": len(X_test_split),
                    "leakage_type": "none",
                    "note": f"Template-specific model: {len(validated_docs)} unique files verified in database",
                }
            else:
                print(f"\nâ­ï¸  Skipping leakage check (using cached results)")
                cached = validation_strategy.get_cached_results()
                leakage_results = cached.get(
                    "leakage_results", {"leakage_detected": False}
                )

            # Save validation results to cache
            validation_strategy.save_validation_results(
                num_samples=len(X_train),
                diversity_metrics=diversity_metrics,
                leakage_results=leakage_results,
            )
        else:
            print(f"\nâ­ï¸  Skipping all validation checks (using cached results)")
            cached = validation_strategy.get_cached_results()
            diversity_metrics = cached.get("diversity_metrics", {})
            leakage_results = cached.get("leakage_results", {"leakage_detected": False})

            # âœ… ALWAYS SPLIT: Even when validation skipped, split for test metrics
            print(f"\nðŸ“Š Splitting data into train/test sets...")
            X_train_split, X_test_split, y_train_split, y_test_split = (
                split_training_data(X_train, y_train, test_size=0.2, random_state=42)
            )

        # âœ… GET RECOMMENDATIONS (before saving to database)
        recommendations = get_training_recommendations(
            num_samples=len(X_train),
            diversity_score=diversity_metrics.get("diversity_score", 0.5),
            has_leakage=leakage_results.get("leakage_detected", False),
            leakage_type=leakage_results.get("leakage_type", "unknown"),
            template_specific=True,  # âœ… This is a template-specific model
        )

        # âœ… SAVE TO DATABASE: Save metrics (both when validated and when skipped)
        validation_duration = time.time() - validation_start_time
        dq_repo = DataQualityRepository(self.db)
        metric_id = dq_repo.save_metrics(
            template_id=template_id,
            validation_type="training",
            total_samples=len(X_train),
            diversity_metrics=diversity_metrics,
            leakage_results=leakage_results,
            recommendations=recommendations,
            validation_duration=validation_duration,
            train_samples=len(X_train_split) if X_train_split else None,
            test_samples=len(X_test_split) if X_test_split else None,
            triggered_by="training",
            notes=f"Training session - {validation_decision['reason']}",
        )
        print(f"   ðŸ’¾ Metrics saved to database (ID: {metric_id})")

        # Remove duplicate save logic
        if False and not validation_decision["should_validate"]:
            validation_duration = time.time() - validation_start_time
            dq_repo = DataQualityRepository(self.db)
            metric_id = dq_repo.save_metrics(
                template_id=template_id,
                validation_type="training",
                total_samples=len(X_train),
                diversity_metrics=diversity_metrics,
                leakage_results=leakage_results,
                recommendations=recommendations,
                validation_duration=validation_duration,
                train_samples=len(X_train),
                test_samples=0,
                triggered_by="training",
                notes=f"Training session - {validation_decision['reason']} (using cached validation)",
            )
            print(f"   ðŸ’¾ Metrics saved to database (ID: {metric_id})")

        print(f"\nðŸ’¡ Training Recommendations:")
        for rec in recommendations:
            print(f"   {rec}")

        # Initialize or load existing model
        model_path = os.path.join(model_folder, f"template_{template_id}_model.joblib")

        learner = AdaptiveLearner(model_path if os.path.exists(model_path) else None)

        # âœ… Train model on TRAINING SET only
        print(f"\nðŸŽ“ Training model on {len(X_train_split)} samples...")
        metrics = learner.train(X_train_split, y_train_split)

        # âœ… Evaluate on TEST SET (unseen data)
        print(f"\nðŸ“Š Evaluating on {len(X_test_split)} test samples...")
        test_metrics = learner.evaluate(X_test_split, y_test_split)

        print(f"\nðŸ“ˆ Results Comparison:")
        print(f"   Training Accuracy: {metrics.get('accuracy', 0)*100:.2f}%")
        print(f"   Test Accuracy:     {test_metrics.get('accuracy', 0)*100:.2f}%")
        print(
            f"   Difference:        {abs(metrics.get('accuracy', 0) - test_metrics.get('accuracy', 0))*100:.2f}%"
        )

        if abs(metrics.get("accuracy", 0) - test_metrics.get("accuracy", 0)) > 0.1:
            print(f"   âš ï¸  WARNING: Large gap between train/test accuracy!")
            print(f"       This indicates overfitting. Model memorized training data.")
        else:
            print(f"   âœ… Good generalization. Model should work on new data.")

        # Save model
        learner.save_model(model_path)

        # Mark feedback as used
        self.feedback_repo.mark_as_used(feedback_ids)

        # Save training history with BOTH train and test metrics
        self.training_repo.create(
            template_id=template_id,
            model_path=model_path,
            training_samples=len(X_train_split),
            metrics=test_metrics,  # âœ… Save TEST metrics (real performance)
        )

        return {
            "template_id": template_id,
            "model_path": model_path,
            "training_samples": len(X_train_split),
            "test_samples": len(X_test_split),
            "train_metrics": metrics,
            "test_metrics": test_metrics,  # âœ… Real performance on unseen data
            "diversity_metrics": diversity_metrics,
            "leakage_detected": leakage_results["leakage_detected"],
            "recommendations": recommendations,
        }

    def get_training_history(self, template_id: int) -> List[Dict]:
        """Get training history for a template"""
        return self.training_repo.find_training_history(template_id)

    def get_latest_metrics(self, template_id: int) -> Optional[Dict]:
        """
        Get latest model metrics for a template

        Args:
            template_id: Template ID

        Returns:
            Latest metrics or None if no history
        """
        history = self.training_repo.find_training_history(template_id)

        if not history:
            return None

        latest = history[0]  # Most recent training

        return {
            "template_id": template_id,
            "metrics": {
                "accuracy": latest["accuracy"],
                "precision": latest["precision_score"],
                "recall": latest["recall_score"],
                "f1": latest["f1_score"],
                "training_samples": latest["training_samples"],
                "trained_at": latest["trained_at"],
            },
        }

    def get_feedback_statistics(self, template_id: int) -> Dict[str, Any]:
        """
        Get feedback statistics for a template

        Args:
            template_id: Template ID

        Returns:
            Feedback statistics
        """
        all_feedback = self.feedback_repo.find_for_training(
            template_id, unused_only=False
        )
        unused_feedback = self.feedback_repo.find_for_training(
            template_id, unused_only=True
        )

        return {
            "template_id": template_id,
            "stats": {
                "total_feedback": len(all_feedback),
                "unused_feedback": len(unused_feedback),
                "used_feedback": len(all_feedback) - len(unused_feedback),
            },
        }

    def trigger_adaptive_learning(
        self,
        document_id: int,
        all_fields: Dict[str, str],
        corrected_fields: Dict[str, str],
    ) -> Dict[str, Any]:
        """
        Trigger adaptive pattern learning after validation

        This method learns from BOTH:
        1. Corrected fields (wrong extractions that were fixed)
        2. Non-corrected fields (correct extractions that were validated)

        Args:
            document_id: Document ID
            all_fields: All extracted fields (original extraction)
            corrected_fields: Only fields that were corrected

        Returns:
            Dict with learning trigger results
        """
        from core.learning.auto_pattern_learner import get_auto_learner

        # Get document info
        doc = self.document_repo.find_by_id(document_id)
        if not doc:
            return {"success": False, "error": f"Document {document_id} not found"}

        template_id = doc["template_id"]
        auto_learner = get_auto_learner(self.db)

        # Track learning triggers
        triggered_fields = []
        skipped_fields = []
        errors = []

        # Learn from ALL fields (both corrected and non-corrected)
        for field_name in all_fields.keys():
            try:
                # Check if learning should be triggered
                if auto_learner.should_trigger_learning(template_id, field_name):
                    # Trigger learning in background
                    auto_learner.trigger_learning(
                        template_id=template_id, field_name=field_name, async_mode=True
                    )

                    # Mark whether this field was corrected or validated
                    is_corrected = field_name in corrected_fields
                    triggered_fields.append(
                        {
                            "field_name": field_name,
                            "type": "corrected" if is_corrected else "validated",
                        }
                    )
                else:
                    skipped_fields.append(field_name)

            except Exception as e:
                errors.append({"field_name": field_name, "error": str(e)})

        return {
            "success": True,
            "document_id": document_id,
            "template_id": template_id,
            "triggered_fields": triggered_fields,
            "skipped_fields": skipped_fields,
            "errors": errors,
            "summary": {
                "total_fields": len(all_fields),
                "triggered": len(triggered_fields),
                "skipped": len(skipped_fields),
                "errors": len(errors),
            },
        }
